{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff521a3a-42e6-4586-8e58-460e4554420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python 320\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 573ms/step - accuracy: 0.6257 - loss: 0.6299 - val_accuracy: 0.7696 - val_loss: 0.4888\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 615ms/step - accuracy: 0.8522 - loss: 0.3602 - val_accuracy: 0.8142 - val_loss: 0.4059\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 770ms/step - accuracy: 0.8657 - loss: 0.3179 - val_accuracy: 0.8526 - val_loss: 0.3757\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 742ms/step - accuracy: 0.8894 - loss: 0.2850 - val_accuracy: 0.8308 - val_loss: 0.4064\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 674ms/step - accuracy: 0.8874 - loss: 0.2838 - val_accuracy: 0.8574 - val_loss: 0.3510\n",
      "Test Accuracy: 86.14%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n        Explanation:\\n        evaluate(): Computes loss and accuracy on unseen test data.\\n        test_acc: Accuracy reflects how well the model generalizes to new reviews.\\n        Typical Output: ~80-88% accuracy depending on hyperparameters.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Explanation:\n",
    "\"\"\" \n",
    "        Sequential: A linear stack of layers to build the model.\n",
    "        Embedding: Converts integer-encoded words into dense vectors (e.g., \"cat\" → [0.2, -0.5, ...]).\n",
    "        LSTM: Layer to process sequential data with memory cells and gates.\n",
    "        Dense: Fully connected layer for classification.\n",
    "        imdb: Preloaded dataset of movie reviews labeled as positive (1) or negative (0).\n",
    "        sequence: Utilities for padding sequences to a fixed length. \n",
    "\"\"\"\n",
    "\n",
    "# Load dataset\n",
    "vocab_size = 5000  # Use top 5,000 frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "\"\"\"\n",
    "        Explanation:\n",
    "        vocab_size=5000: Restrict vocabulary to the 5,000 most frequent words (reduces noise from rare words).\n",
    "        imdb.load_data(): Loads the IMDB dataset preprocessed into integer sequences.\n",
    "        x_train/x_test: Lists of reviews, where each word is replaced by its integer index.\n",
    "        y_train/y_test: Labels (0 or 1).\n",
    "\"\"\"\n",
    "\n",
    "# Pad sequences to fixed length (400 words)\n",
    "max_words = 400\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "\"\"\"\n",
    "        Explanation:\n",
    "        max_words=400: Truncate/pad all reviews to 400 words.\n",
    "        Shorter reviews are padded with zeros (e.g., [0, 0, ..., 12, 42]).\n",
    "        Longer reviews are truncated to 400 words.\n",
    "        Why? Neural networks require fixed-length inputs for batch processing.\n",
    "\"\"\"\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential(name=\"LSTM_Sentiment_Analysis\")\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_words))\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=False))  # 128 LSTM units\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\"\"\"\n",
    "        Step 1: Embedding Layer:\n",
    "        vocab_size: 5,000 unique words.\n",
    "        32: Embedding dimension (each word is a 32-dimensional vector).\n",
    "        input_length=max_words: Each input sequence has 400 words.\n",
    "        Purpose: Converts sparse integer-encoded words into dense vectors that capture semantic meaning (e.g., \"good\" and \"great\" are closer in vector space).\n",
    "\n",
    "        Step 2: LSTM Layer:\n",
    "        128: Number of LSTM units (dimensionality of the hidden state).\n",
    "        activation='tanh': Hyperbolic tangent activation for gate updates.\n",
    "        return_sequences=False: Return only the final output (not all timesteps).\n",
    "        Purpose: Processes the sequence word-by-word, updating its hidden state to capture context.\n",
    "\n",
    "        Step 3: Dense Layer:\n",
    "        1: Single neuron for binary classification (positive/negative).\n",
    "        activation='sigmoid': Squashes output to [0, 1] (probability of positive sentiment).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "        Explanation:\n",
    "        loss='binary_crossentropy': Standard loss for binary classification.\n",
    "        optimizer='adam': Adaptive learning rate optimizer (efficient for RNNs).\n",
    "        metrics=['accuracy']: Track accuracy during training.\n",
    "\"\"\"\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_train, y_train, \n",
    "                   batch_size=64, \n",
    "                   epochs=5, \n",
    "                   validation_split=0.2)\n",
    "\"\"\"\n",
    "        Explanation:\n",
    "        batch_size=64: Update weights after every 64 samples (balance speed/memory).\n",
    "        epochs=5: Train for 5 full passes over the training data.\n",
    "        validation_split=0.2: Use 20% of training data for validation (monitor overfitting).\n",
    "        Output: Training logs show loss/accuracy for training and validation sets.\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\"\"\"\n",
    "        Explanation:\n",
    "        evaluate(): Computes loss and accuracy on unseen test data.\n",
    "        test_acc: Accuracy reflects how well the model generalizes to new reviews.\n",
    "        Typical Output: ~80-88% accuracy depending on hyperparameters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cebc0f7-3d3e-4fde-8554-68bb77e21c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
       "         13, 1228,  119,   14,  552,    7,   20,  190,   14,   58,   13,\n",
       "        258,  546, 1786,    8, 1968,    4,  268,  237,   13,  191,   81,\n",
       "         15,   13,   80,   43, 3824,   44,   12,   14,   16,  427, 3192,\n",
       "          4,  183,   15,  593,   19,    4,  351,  362,   26,   55,  646,\n",
       "         21,    4, 1239,   84,   26, 1557, 3755,   13,  244,    6, 2071,\n",
       "        132,  184,  194,    5,   13,   70, 4478,  546,   73,  190,   13,\n",
       "         62,   24,   81,  320,    4,  538,    4,  117,  250,  127,   11,\n",
       "         14,   20,   82,    4,  452,   11,   14,   20,    9,    2,   19,\n",
       "         41,  476,    8,    4,  213,    7,    2,   13,  657,   13,  286,\n",
       "         38, 1612,   44,   41,    5,   41, 1729,   88,   13,   62,   28,\n",
       "        900,  510,    4,  509,   51,    6,  612,   59,   16,  193,   61,\n",
       "       4666,    5,  702,  930,  143,  285,   25,   67,   41,   81,  366,\n",
       "          4,  130,   82,    9,  259,  334,  397, 1195,    7,  149,  102,\n",
       "         15,   26,  814,   38,  465, 1627,   31,   70,  983,   67,   51,\n",
       "          9,  112,  814,   17,   35,  311,   75,   26,    2,  574,   19,\n",
       "          4, 1729,   23,    4,  268,   38,   95,  138,    4,  609,  191,\n",
       "         75,   28,  314, 1772])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa84b71c-8fcc-48b3-a0a9-0198395b0885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 118ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities on test data\n",
    "y_pred_prob = history.model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697de318-bf42-46a9-9184-91dd3b67111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d46ad0d-f500-4270-ac41-330e5ff3ff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [0 1 0 0 1 1 1 0 1 1]\n",
      "Actual labels:    [0 1 1 0 1 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted labels:\", y_pred[:10].flatten())\n",
    "print(\"Actual labels:   \", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde73438-fb85-4eb6-bee5-7c962d2fc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078ca95-6fe7-4eb3-bebe-a02f2cf82396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
